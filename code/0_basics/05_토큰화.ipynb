{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이제이션 (토큰화 : Tokenizaiton)\n",
    "- 토큰화 : 문장을 단어 별로 나누는 것\n",
    "- 수치화 : 토큰을 수치화 (벡터화) 하는 과정\n",
    "\n",
    "- 벡터화\n",
    "    - 원 핫 인코딩\n",
    "        - 노미널 카테고리컬 데이터와 같은 방식으로 처리하고 \n",
    "        - 각 단어가 인덱스이고 해당 단어는 1이고 그렇지 않은 단어는 0으로 표현 \n",
    "        - 단어의 수가 많아질 수록 비효율적이다. sparce 하다.\n",
    "    - 단어 임베딩\n",
    "        - 각 단어를 시퀀스화 하여 번호를 매기고 벡터화 하여 단어 간의 의미관계를 학습\n",
    "        - 토큰화 → 불용어 발라내기 → 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB를 이용한 영화평"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import torch\n",
    "\n",
    "# 만개의 단어를 이용\n",
    "max_features = 10000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 최대 길이\n",
    "max_len = 200 \n",
    "\n",
    "# 패딩 : 최대 길이보다 길경우 자르고, 짧을 경우 0으로 채운다.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           640000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                409632    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,049,665\n",
      "Trainable params: 1,049,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "good_bad_model = Sequential()\n",
    "\n",
    "# 임베딩 과정 (벡터라이제이션) : 64개의 벡터값들을 초기는 랜덤하게 초기값을 한다.\n",
    "# 이 초기값은 이를 학습하며 파라미터들이 업데이트 한다.-> 단어간의 의미관계 반영\n",
    "# (총 데이터의 갯수, 200, 64) = (200 : 문서의 길이, 64: 벡터의 차원)\n",
    "good_bad_model.add(Embedding(max_features, 64, input_length=max_len))\n",
    "\n",
    "# 데이터별로 1D 로 만들어 각 노드별로 계산할수 있는 형태로 만든다.\n",
    "# 총 2 차원 (데이터 포인트의 수, max_len * 임베딩 차원)\n",
    "good_bad_model.add(Flatten())\n",
    "\n",
    "# 학습 레이어 만들기\n",
    "good_bad_model.add(Dense(32, activation = 'relu'))\n",
    "good_bad_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# 컴파일 과정\n",
    "good_bad_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 확인\n",
    "good_bad_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 4s 6ms/step - loss: 0.4455 - accuracy: 0.7681 - val_loss: 0.2969 - val_accuracy: 0.8728\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.1075 - accuracy: 0.9644 - val_loss: 0.3405 - val_accuracy: 0.8624\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0131 - accuracy: 0.9984 - val_loss: 0.3885 - val_accuracy: 0.8668\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.4249 - val_accuracy: 0.8654\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 9.6459e-04 - accuracy: 1.0000 - val_loss: 0.4443 - val_accuracy: 0.8668\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 5.5311e-04 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.8672\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 3.6795e-04 - accuracy: 1.0000 - val_loss: 0.4781 - val_accuracy: 0.8676\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 2.5402e-04 - accuracy: 1.0000 - val_loss: 0.4915 - val_accuracy: 0.8682\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.8550e-04 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.8684\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.3826e-04 - accuracy: 1.0000 - val_loss: 0.5189 - val_accuracy: 0.8678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18ca7906040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_bad_model.fit(x_train,y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size= 64,\n",
    "                   validation_split = 0.2,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01089439, -0.01490348,  0.00304474, -0.01618349,  0.002664  ,\n",
       "       -0.02977126,  0.00848474, -0.01083722,  0.00460668, -0.01055055,\n",
       "        0.01836477,  0.01115856, -0.0081366 , -0.00030645,  0.0322329 ,\n",
       "        0.01768994, -0.00036912,  0.01530584,  0.00323665,  0.00723391,\n",
       "       -0.00680686, -0.00878318, -0.0235143 , -0.00043743, -0.00125735,\n",
       "       -0.00958201,  0.00223411,  0.00856254,  0.01542935, -0.00662668,\n",
       "        0.00643415, -0.00391647,  0.00358173,  0.00748805,  0.01662424,\n",
       "        0.01216564,  0.00366573, -0.01816105,  0.01987887, -0.02183816,\n",
       "       -0.0120273 ,  0.00837164, -0.00835055,  0.0019806 ,  0.0267897 ,\n",
       "       -0.00565639,  0.00270927,  0.01105821, -0.00983608,  0.00172654,\n",
       "       -0.01557806, -0.0063318 ,  0.00421915, -0.01677806, -0.00836237,\n",
       "        0.0051714 ,  0.00444504,  0.01861977,  0.01789569,  0.00339429,\n",
       "       -0.00541311,  0.02155936,  0.01253655,  0.01446076], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어별 임베딩\n",
    "# 0 ~ 9999 까지\n",
    "good_bad_model.get_weights()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = good_bad_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_= [1 if i > 0.5 else 0 for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86764"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = (pred_ == y_test)\n",
    "correct.sum() / len(correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
