{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝\n",
    "- 신경망, 비 선형 문제에서 좀 더 장점을 보인다.\n",
    "- **활성화 함수 : 비선형 문제의 핵심**\n",
    "> 비교\n",
    "> - 전통적인 머신러닝\n",
    ">   - 선형적인 분포와 가정을 기반한다.\n",
    ">   - 특징 추출 과정에서 도메인 지식을 요한다.\n",
    ">   - 선택한 피쳐의 퀄리티에 크게 의존"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 종류\n",
    "|카테고리|모델 이름|설명|\n",
    "|---|---|---|\n",
    "| 지도 학습 모델| FNN (Feedforward Neural Networks) | 입력 데이터를 한 방향으로 전파하여 결과를 예측하는 기본적인 형태의 신경망.|\n",
    "|| CNN (Convolutional Neural Networks) | 주로 이미지 처리에 사용되는 신경망으로, 합성곱과 풀링 계층을 통해 특징을 추출. |\n",
    "| 순환신경망| RNN (Recurrent Neural Networks)| 시계열 데이터나 자연어 처리와 같은 연속적인 데이터를 처리하기 위해 고안된 신경망.|\n",
    "|| LSTM (Long Short-Term Memory) | RNN의 일종으로, 장기 의존성을 처리하기 위해 셀 상태와 게이트 메커니즘을 포함. |\n",
    "|| GRU (Gated Recurrent Unit)| LSTM의 경량 버전으로, 유사한 성능을 제공하면서도 계산 효율성이 높음. |\n",
    "| 자연어 처리 및 시퀀스 모델 | Seq2Seq| 입력 시퀀스를 다른 형태의 출력 시퀀스로 변환하는 모델 구조. 주로 번역 등에 사용.|\n",
    "|| LM (Language Models)| 텍스트 데이터를 학습하여 언어의 맥락을 이해하고 다음 단어를 예측하는 모델. |\n",
    "| 사전 학습 및 전이 학습 | 사전 학습 모델| 대량의 데이터로 사전 학습된 모델을 말하며, 보통 다른 태스크로 전이 학습에 활용. |\n",
    "|| 전이 학습| 기존에 학습된 모델을 새로운 유사 태스크에 재사용하는 방법.|\n",
    "| 오브젝트 검출| YOLO, SSD, R-CNN| 이미지에서 객체를 검출하는 모델들로, 실시간 성능과 정확도 측면에서 다양한 특성을 제공. |\n",
    "| Transformer 기반 모델 | Transformer| \"Attention is All You Need\" 논문에서 소개된 구조로, 셀프 어텐션 메커니즘을 사용하여 병렬 처리 가능.|\n",
    "|| BERT (Bidirectional Encoder Representations from Transformers) | 양방향으로 텍스트 컨텍스트를 이해하는 모델로, 다양한 태스크에 높은 성능을 보임. |\n",
    "|| GPT (Generative Pre-trained Transformer) | 사전 학습된 텍스트 생성 모델로, 다양한 자연어 생성 및 이해 태스크에 활용.|\n",
    "| 자가 지도 학습 | AE (Autoencoder)| 입력을 자체적으로 코딩하고 디코딩하는 방식으로 학습하는 모델로, 차원 축소나 특징 학습에 사용.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝의 구조\n",
    "- 노드 (Node, 뉴런)\n",
    "  - 역할 :\n",
    "    - 신호 처리 : 곱과 합을 이용\n",
    "    - 비 선형성 : 활성화 함수 이용\n",
    "    - 정보 통합 : 곱하여 더하므로 정보를 통합\n",
    "  - 정보를 받아 입 출력하는 기본단위.\n",
    "  - 여러 입력을 받아 하나의 출력을 생성\n",
    "\n",
    "- 가중치 (weights)\n",
    "  - 역할 :\n",
    "    - 입력 중요도 : 각 피쳐 별 중요도 부여\n",
    "    - 학습 : 가중치를 업데이트 하며 학습한다.\n",
    "    - 신호 전달\n",
    "  - 각 연결선에 할당.\n",
    "  - 영향을 끼치는 정도\n",
    "\n",
    "- 편향 노드 (bias node)\n",
    "  - 선형 회귀에서의 절편 : 즉 평행 이동, 모델 일반화에 큰 도움을 준다\n",
    "  - 활성화 함수가 결정하는 임계 값 이상일 때 노드를 활성화 한다.\n",
    "  - (바이어스 값이 노드의 활성화를 조절한다.)\n",
    "  - 활성화 임계를 조정\n",
    "    편향을 통해 모델은 입력 데이터가 없거나 0에 가까울 때도 활성화 <br>\n",
    "    -> 모델의 학습 능력과 일반화 능력 향상\n",
    "\n",
    "\n",
    "- 레이어\n",
    "  - 입력 레이어 및 출력 레이어 : 각 1층 씩\n",
    "  - 은닉층은 설계자의 결정\n",
    "  - 레이어 별 이름\n",
    "      - Single Layer Perceptron : 단층, 은닉 층 없음\n",
    "      - Multi Layer Perceptron : 다층, 은닉 층 있음\n",
    "      - Shallow Neural Net : 얕은 신경망, 은닉 층은 하나\n",
    "      - Deep Neural Net : 심층 신경망, 은닉 층은 두개 이상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 단층 퍼셉트론 및 다층 퍼셉트론\n",
    "\n",
    "- 단층 퍼셉트론의 한계점 : XOR 같은 비선형 문제를 해결 불가능 하다. 실제 세상은 대부분 비선형\n",
    "- 단층 퍼셉트론 에서의 역 전파? : 역 전파의 개념이 생긴 시점은 다층 퍼셉트론 부터 이다.\n",
    "- 다층 퍼셉트론의 특징 : 비선형성은 활성화 함수로 표현 된다\n",
    "    > (**범용 근사자 : 충분히 크고 복잡한 문제라도 이론적으로 학습이 가능하다.**) - 역 전파로 오류를 최소화 한다.\n",
    "\n",
    "### 인공 신경망의 프로세스\n",
    "\n",
    "- 프로그래밍 적 관점 :\n",
    "    - 데이터 세팅\n",
    "    - 독립 변수 및 종속변수 관계 파악 및 비용함수 최소화\n",
    "    - 모델 성능 평가\n",
    "    - 종속 변수의 예측\n",
    "- 내부 거동의 관점 :\n",
    "    - Weights 및 biases 초기화\n",
    "    - 순 전파\n",
    "    - 오차 계산\n",
    "    - 오류 함수의 역 전파 \n",
    "    - 가중치 업데이트\n",
    "    - 과 적합 및 정규화 : L1 및 L2 정규화, 드롭 아웃 등\n",
    "    - 반복 학습\n",
    "\n",
    "### 선형 회귀 모형과 신경망 모델의 차이\n",
    "\n",
    "- 선형 회귀 모형 : $\\hat{y} = b_0 + b_1  X_1 +  b_2  X_2 + b_3  X_3 + b_4  X_4$\n",
    "- 위의 선형 회귀 모형과 신경망 모델의 차이 : 은닉 층의 존재 여부\n",
    "\n",
    "### 출력 노드의 수\n",
    "\n",
    "- 회귀 문제 : 1개\n",
    "- 분류 문제 : 정답 라벨의 개 수."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
