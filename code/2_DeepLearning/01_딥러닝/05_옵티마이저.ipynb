{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "- SGD (Stochastic Gradient Descent)\n",
    "- Momentum\n",
    "- NAG(Nestrov Accelerated Gradient)\n",
    "- Adagrad\n",
    "- RMSProp\n",
    "- Adam (Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Optimizer\n",
    "\n",
    "- 딥 러닝의 학습 과정 중, 손실함수의 거리를 좁혀가는 함수.\n",
    "    - 이 과정에서 미분을 이용한 수치 최적화 알고리즘을 사용한다. - 여기서 **수치 최적화 알고리즘**을 **Optimizer**라고 한다.\n",
    "\n",
    "## 1-2. 수치 최적화 알고리즘\n",
    "\n",
    "- 반복적으로 정해진 방법을 업데이트 하는 방식을 사용하여 문제를 푼다.\n",
    "- 초기값 설정, 방향 설정, 학습 율 만큼 업데이트 한다.\n",
    "\n",
    "# 2. 수치 최적화 알고리즘 종류\n",
    "\n",
    "| 알고리즘 | 연도 | 학습률 | 탐색방향 | 알고리즘 기반 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| SGD | 1945 | 상수 | 그래디언트 | 탐색방향 |\n",
    "| Momentom/\n",
    "Nesterov | 1964/\n",
    "1983 | 상수 | 단기 누적 그래디언트 | 탐색방향 |\n",
    "| Adagrad | 2011 | 장기 파라미터 변화량과 반비례 | 그래디언트 | 학습률 |\n",
    "| RMSProp | 2012 | 단기 파라미터 변화량과 반비례 | 그래디언트 | 학습률 |\n",
    "| Adam | 2014 | 단기 파라미터 변화량과 반비례 | 단기 누적 그래디언트 | 학습률 |\n",
    "\n",
    "## 2-1 탐색 방향 기반 알고리즘\n",
    "\n",
    "- Stocastic Gredient Descent (SGD) - 확률적 미니배치 경사 하강 법, 계산 량이 너무 많다. 그래서 Mini batch를 이용하여 랜덤 추출하여 그래디언트 계산을 한다.\n",
    "    - 과정 :\n",
    "        - 초기값 $w^{(0)}$ 설정\n",
    "        - 데이터 섞기\n",
    "        - mini batch 만큼 추출 후 계산 반복\n",
    "            - $\\nabla f(w^{(k)}) = \\frac{\\partial}{\\partial w} f(w,b)$ : 가중치와 바이어스 값 에대한 Loss의 편 미분\n",
    "            - 탐색 방향 = $-\\mu \\nabla f(w^{w(k)})$ : 탐색방향으로 학습률 $\\mu$ 만큼 한걸음 내딛는다.\n",
    "            - 파라미터 업데이트 $w^{(k+1)} \\leftarrow w^{(k)} - \\mu \\nabla f(w^{(k)})$\n",
    "- Momentum : Gredient 의 국소 최소 값을 벗어 나기위한 방법 ( 전 단계의 탐색 방향 누적 합에 현재 GD를 더해 벗어 나게 해준다. ) 즉 속도의 개념을 추가하여 도랑을 지나 넘어가게 해준다.\n",
    "    - 과정 :\n",
    "        - 초기값 $w^{(0)}$ 설정\n",
    "        - 초기 누적 속도 $v^{(0)} = 0$ 설정\n",
    "        - 데이터 섞기\n",
    "        - mini batch 만큼 추출 후 계산 반복\n",
    "            - 누적 속도 업데이트 $v^{(k+1)} \\leftarrow \\alpha v^{(k)} - \\mu \\nabla f(w^{(k)})$\n",
    "            - 파라미터 업데이트 $w^{(k+1)} \\leftarrow w^{(k)} +v^{(k+1)}$\n",
    "- Nesterov : Momentom 보다 수렴 속도가 빠르다(그래디언트 부분의 차이만 있다)\n",
    "    - 과정 :\n",
    "        - 초기값 $w^{(0)}$ 설정\n",
    "        - 초기 누적 속도 $v^{(0)} = 0$ 설정\n",
    "        - 데이터 섞기\n",
    "        - mini batch 만큼 추출 후 계산 반복\n",
    "            - 속도에 대한 미분(가속도 개념) 계산 :  $\\nabla f(w^{(k)} + \\alpha v^{(k)})$\n",
    "            - 누적 속도 업데이트 $v^{(k+1)} \\leftarrow \\alpha v^{(k)} - \\mu$ **$\\nabla f(w^{(k)} + \\alpha v^{(k)})$**\n",
    "            - 파라미터 업데이트 $w^{(k+1)} \\leftarrow w^{(k)} +v^{(k+1)}$\n",
    "\n",
    "## 2-2 학습률 기반 알고리즘\n",
    "\n",
    "- 학습률을 고정하지 않고 적절한 학습률 계산 ( 학습률 고정 시 발산, 속도 느림 문제가 있음 : SGD, Momentom, Nesterov)\n",
    "    - 처음엔 큰 학습률로 시작하여 손실 함수 값이 작아지면 학습률 크기를 절반으로 하여 업데이트 하는 방식이다.\n",
    "- Adagrad : 최솟값을 잘 찾아간다. 초기부터 미분의 제곱의 누적의 합을 가지고 있어 속도가 느림\n",
    "    - 과정:\n",
    "        - 초기값 $w^{(0)}$ 설정\n",
    "        - 장기 누적 미분 $r^{(0)} = 0$ 설정\n",
    "        - 데이터 섞기\n",
    "        - mini batch 만큼 추출 후 계산 반복\n",
    "            - 미분 계산 $\\nabla f ( w^{(k)} )$\n",
    "            - 장기 누적 미분 계산 : $r^{(k+1)} \\leftarrow  r^{(k)}  + \\nabla f ( w^{(k)} ) \\odot \\nabla f ( w^{(k)} )$\n",
    "                \n",
    "                여기서 $\\odot$ 은 각 요소의 곱, 즉 여기서는 각 요소가 같으므로 제곱이다.\n",
    "                \n",
    "            - 파라미터 업데이트 : $w^{(k+1)} \\leftarrow w^{(k)} - \\frac{\\mu}{\\delta + \\sqrt{r^{(k+1)}}} \\odot \\nabla f ( w^{(k)} )$\n",
    "                \n",
    "                여기서 $\\delta$ 분모가 0이 되기를 방지하는 아주 작은 상수이다.  \n",
    "                \n",
    "- RMSProop (Root Mean Square Propagation) : 오래된 미분은 반영이 적어진다. $\\rho$ 와 $1- \\rho$  로 인해 초기 가중치의 영향이 줄어든다.\n",
    "    - 과정 :\n",
    "        - 초기값 $w^{(0)}$ 설정\n",
    "        - 장기 누적 미분 $r^{(0)} = 0$ 설정\n",
    "        - 데이터 섞기\n",
    "        - mini batch 만큼 추출 후 계산 반복 **(빨간 글씨는 adagrad 와의 다른점)**\n",
    "            - 미분 계산 $\\nabla f ( w^{(k)} )$\n",
    "            - 장기 누적 미분 계산 : $r^{(k+1)} \\leftarrow$ $\\rho$  $r^{(k)}  + ($ $1 - \\rho$ $) \\nabla f ( w^{(k)} ) \\odot \\nabla f ( w^{(k)} )$\n",
    "            - 파라미터 업데이트 : $w^{(k+1)} \\leftarrow w^{(k)} - \\frac{\\mu}{\\delta + \\sqrt{r^{(k+1)}}} \\odot \\nabla f ( w^{(k)} )$\n",
    "- **Adam(Adaptive Moment)** : 탐색 방향 + 학습률 갱신\n",
    "    - 과정 :\n",
    "        - 초기값 $w^{(0)}$ 설정\n",
    "        - 단기 누적 미분의 크기 $r^{(0)} = 0$ 설정\n",
    "        - 단기 누적 미분의 합 $s^{(0)} = 0$ 설정\n",
    "        - 데이터 섞기\n",
    "        - mini batch 만큼 추출 후 계산 반복\n",
    "            - 미분 계산 $\\nabla f ( w^{(k)} )$\n",
    "            - 탐색 방향 (평균 : 단기 누적 미분의 합 계산):\n",
    "                - $s^{(k+1)} \\leftarrow \\rho_1 s^{(k)}  + (1 - \\rho_1) \\nabla f ( w^{(k)} )$\n",
    "                - $\\hat{s}^{(k+1)} \\leftarrow \\frac{s^{(k+1)}}{1-\\rho_1^{(k+1)}}$\n",
    "            - 학습률 (분산 : 단기 누적 미분의 크기 ) :\n",
    "                - $r^{(k+1)} \\leftarrow \\rho_2 r^{(k)} + (1-\\rho_2) \\nabla f(w^{(k)}) \\odot \\nabla f(w^{(k)})$\n",
    "                - $\\hat{r}^{(k+1)} \\leftarrow \\frac{r^{(k+1)}}{1-\\rho_2^{(k+1)}}$\n",
    "            - 파라미터 업데이트 : $w^{(k+1)} \\leftarrow w^{(k)} - \\frac{\\mu}{\\delta + \\sqrt{\\hat{r}^{(k+1)}}} \\hat{s}^{(k+1)}$\n",
    "- Adabelif : adaptive methods 의 빠른 수렴 성, SGD 와 같은 일반화, 훈련 안정성 - 관측된 기울기가 예측과 가까우면 큰 걸음을, 관측된 기울기가 예측과 다르면 작은 걸음을 내딛는다. (기울기를 믿고 도약)\n",
    "    - 이미지 분류 및 언어 모델에서 빠른 수렴과 높은 정확도.\n",
    "    - ImageNet 에서 SGD 와 비슷한 정확도\n",
    "    - Cifar10데이터 셋의 GAN 훈련에 대한 높은 안정성\n",
    "    - Adam 과의 차이점\n",
    "        - 미분 계산 $\\nabla f ( w^{(k)} )$\n",
    "        - 탐색 방향 : 단기 누적 미분의 합 계산 :\n",
    "            - $s^{(k+1)} \\leftarrow \\rho_1 s^{(k)}  + (1 - \\rho_1) \\nabla f ( w^{(k)} )$\n",
    "            - $\\hat{s}^{(k+1)} \\leftarrow \\frac{s^{(k+1)}}{1-\\rho_1^{(k+1)}}$\n",
    "        - 학습 률 ( 단기 누적 미분의 크기 ) : **파란부분은 Adam 과의 차이점**\n",
    "            - $r'^{(k+1)} \\leftarrow \\rho_2 r'^{(k)} + (1-\\rho_2)$ $(\\nabla f(w^{(k)}) - s^{(k+1)}) \\odot (\\nabla f(w^{(k)}) - s^{(k+1)}) + \\delta$\n",
    "            \n",
    "            라고 할 때\n",
    "            \n",
    "            - $r'^{(k+1)} \\leftarrow \\rho_2 r'^{(k)} + (1-\\rho_2)$$(\\nabla f(w^{(k)}) - s^{(k+1)}) ^2 + \\delta$\n",
    "        - $\\hat{r'}^{(k+1)} \\leftarrow \\frac{r'^{(k+1)}}{1-\\rho_2^{(k+1)}}$\n",
    "        - 파라미터 업데이트 : $w^{(k+1)} \\leftarrow w^{(k)} - \\frac{\\mu}{\\delta + \\sqrt{\\hat{r'}^{(k+1)}}} \\hat{s}^{(k+1)}$\n",
    "    \n",
    "    >다른 옵티마이저와 달리 keras에서 이용 할 시에 `tensorflow_addons` 를 설치해야 한다.      \n",
    "    >\n",
    "    >```python\n",
    "    >!pip install tensorflow-addons\n",
    "    >```\n",
    "    >\n",
    "    >```python\n",
    "    >import tensorflow as tf\n",
    "    >import tensorflow_addons as tfa\n",
    "    >\n",
    "    >adb = tfa.optimizers.AdaBelief()\n",
    "    >```\n",
    "    >"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
