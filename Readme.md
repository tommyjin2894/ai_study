### [Notion ë§í¬](https://royal-offer-53a.notion.site/KDT-2024-05-2024-09-10bf678f80468069b4e1e2f0a631131a?pvs=4)

### [ì „ì²´ íŒŒì¼ êµ¬ì¡°](file_hirachy.md)

### ì°¸ê³  ë§í¬
[roboflow](https://roboflow.com/) <br>
[ultraytics](https://docs.ultralytics.com/integrations/roboflow/) <br>
learn open cv .com <br>
supervisely <br>
superb ai <br>
labelstudio.com -> ì˜¤ë””ì˜¤ì—ì„œ ê°ì„± ë¶„ì„ ê°€ëŠ¥ <br>

### segmentation
[Label Studio](https://labelstud.io/guide/) <br>
[Label Me](https://github.com/labelmeai/labelme) <br>
[Anylabeling](https://github.com/vietanhdev/anylabeling) <br>
[X-Anylabeling](https://github.com/CVHub520/X-AnyLabeling) <br>
 
## ê¸°ë³¸ ì‹œê°í™” ì½”ë“œ
```py
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib_inline.backend_inline

matplotlib_inline.backend_inline.set_matplotlib_formats("png2x") # svg, retina, png2x ...
mpl.style.use("seaborn-v0_8")
mpl.rcParams.update({"figure.constrained_layout.use": True})
sns.set_context("paper") 
sns.set_palette("Set2") 
sns.set_style("whitegrid") 

# ì‹œìŠ¤í…œ í°íŠ¸íŒ¨ë°€ë¦¬ì— ë”°ë¼ ë³€ê²½
plt.rc("font", family = "NanumSquareRound")
plt.rcParams["axes.unicode_minus"] = False
```
<!--------------------------------------->

## íŒŒì´ì¬ ê¸°ë³¸ ì½”ë“œ ì—°ìŠµ

### ë°ì´í„° ë¶ˆê· í˜• ì„±ëŠ¥í‰ê°€
- ë°ì´í„° ë¶ˆê· í˜• íŒë‹¨ê¸°ì¤€ : 30%
    - ë°ì´í„° ë¶ˆ ê· í˜•ì‹œ ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ë° ë‹¤ì–‘í•œ metrics ì„¤ì •
      - ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ìŠ¤ì½”ì–´, AUC-ROC,

### íšŒê·€ ì„±ëŠ¥ í‰ê°€
- RMSE, MAE ë“± 

### ğŸ˜Šê²°ê³¼ë¥¼ í‘œë¡œ ì˜ ì •ë¦¬í•˜ê¸°ğŸ˜Š

<!--------------------------------------->


## ë§ˆì´ë‹ ì•Œê³ ë¦¬ì¦˜

### ë‚´ìš© ì •ë¦¬
ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(ì§€ë„ í•™ìŠµ)
    
|ëª¨ë¸|ì´ë¦„|ì„¤ëª…|
|---|---|---|
|ë¶„ë¥˜|Decision Tree|íŠ¸ë¦¬êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜, ì¡°ê±´ ë¶„ê¸°|
|-|Random Forest|ì•™ìƒë¸” ê¸°ë²•ì¤‘ baseline Bagging ì¤‘ í•˜ë‚˜ <br> ì—¬ëŸ¬ê°œì˜ DTë¡œ êµ¬ì„±|
|-|KNN|ê°€ê¹Œìš´ K ê°œì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ì • <br> baseline L1 ë° L2 ê±°ë¦¬|
|-|SVM|í´ë˜ìŠ¤ ê°„ì˜ ê²½ê³„ë¥¼ ìµœëŒ€í™”í•˜ì—¬ ì´ˆí‰ë©´ì„ ì°¾ëŠ”ë‹¤.|
|íšŒê·€|Linear Regression|ì„ í˜• ê´€ê³„ ëª¨ë¸ë§|
|-|Logistic Regression|ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ íšŒê·€ ë¶„ì„ ê¸°ë²•,<br> baseline í™•ë¥ ë¡œ     ì¶œë ¥ê°’ì„ ë³€í™˜|
|ì¸ê³µ ì‹ ê²½ë§|NN|ì—¬ëŸ¬ì¸µì˜ ë‰´ëŸ°|
|ê¸°íƒ€|AdaBoost|ì•½í•œ í•™ìŠµê¸° x N = ê°•í•œ í•™ìŠµê¸°|
|-|XGBoost|Gradient Boosting Machines ì˜ íš¨ìœ¨ì ì´ê³  ê°•ë ¥í•˜ê²Œ ê°œì„ |


ë¹„ì§€ë„ í•™ìŠµ

 |ì¢…ë¥˜|ì´ë¦„|ì„¤ëª…|
 |-|-|-|
 |í´ëŸ¬ìŠ¤í„°ë§|k-means|ë¹„ìŠ·í•œ í¬ì¸íŠ¸ë¥¼ ê°€ê¹ê²Œ ìœ„ì¹˜|
 |-|ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§|íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ì¡°ì§í™”|
 |ì—°ê´€ ê·œì¹™|Apriori ì•Œê³ ë¦¬ì¦˜|ìì£¼ ë°œìƒ í•˜ëŠ” ì—°ê´€ ì§‘í•©|
 |-|FP-Growth|Apriori ë³´ë‹¤ íš¨ìœ¨ì ì¸ |
 |ì°¨ì› ì¶•ì†Œ|PCA|ë°ì´í„°ë¥¼ ì••ì¶•, ì €ì°¨ì›ìœ¼ë¡œ|
 |-|t-SNE|2~3 ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”, ë¹„ìŠ·í•œ ë°ì´í„° ê·¸ë£¹í™”|

    baseline í´ëŸ¬ìŠ¤í„°ë§ : ìœ ì‚¬ë„ ê¸°ì¤€ L1(manhatten), L2(Euclidean) ìœ¼ë¡œ êµ°ì§‘í™”


ë‹¤ì–‘í•œ ê¸°ë²•

 |ì¢…ë¥˜|ì´ë¦„|ì„¤ëª…|
 |---|---|---|
 |ê¸°ë²•|K-fold êµì°¨ ê²€ì¦|ì ìˆ˜ í‰ê· |
 |-|Grid search|ëª¨ë“  ê²½ìš°ì˜ìˆ˜ë¥¼ ë³¸ë‹¤|
 |-|Randomized search|ëœë¤í•œ ê²½ìš°ì˜ìˆ˜ë¥¼ ë³¸ë‹¤|
 |ì•™ìƒë¸”|bagging<br> (bootstrap aggregating)|1. baseline N ê°œì˜ ìƒ˜í”Œì„ ë½‘ê¸°<br>->ì§‘ì–´ë„£ê³  N ê°œì˜ ìƒ˜í”Œì„ ë½‘ëŠ”ë‹¤. <br> 2. ì¤‘ë³µì´ ìƒê¸¸ ìˆ˜ ìˆìŒ|
 |-|Boosting|ì•½í•œ í•™ìŠµê¸° X N = ê°•í•œ í•™ìŠµê¸° <br>AdaBoost, XGBoost, Lgith GBM, Cat     Boost ë“±|
 |-|Stacking|ì—¬ëŸ¬ ê°œì˜ ê¸°ì´ˆëª¨ë¸ì˜ ì˜ˆì¸¡<br>ì¢…í•©í•˜ì—¬ ìƒˆë¡œìš´ ë©”íƒ€ëª¨ë¸ ìƒì„±|

    <details>
    <summary>K-fold êµì°¨ ê²€ì¦

    - í›ˆë ¨ ë°ì´í„°ë¥¼ k ê°œë¡œ ë¶„í• í•´ ë²ˆê°ˆì•„ ê°€ë©´ì„œ í›ˆë ¨ í‰ê°€
     |í•™ìŠµ ëª¨ë¸|ë°ì´í„°1|ë°ì´í„°2|ë°ì´í„°3|ë°ì´í„°4|ë°ì´í„°5|
     | ---| --- | --- | --- | --- | --- |
     | í•™ìŠµ 1 | train | train | train | train | test |
     | í•™ìŠµ 2 | train | train | train | test | train |
     | í•™ìŠµ 3 | train | train | test | train | train |
     | í•™ìŠµ 4 | train | test | train | train | train |
     | í•™ìŠµ 5 | test | train | train | train | train |

    


### codes

ì „ì²˜ë¦¬

    ```py
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

    # StandardScaler
    model_std = StandardScaler()

    # MinMaxScaler
    model_minmax = MinMaxScaler()

    # RobustScaler
    model_robust = RobustScaler()
    ```


íŠ¸ë ˆì¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 

    ```py
    from sklearn.model_selection import train_test_split

    x_train, x_test, y_train, y_test = train_test_split(
        x_data,y_data,
        test_size=0.3,
        random_state=42,
        )
        # stratify=y_data
        # yë¼ë²¨ì˜ ë¹„ìœ¨ ìœ ì§€
    ```


ë§ˆì´ë‹ ì•Œê³ ë¦¬ì¦˜

    ```py
    # ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
    import sklearn
    # Main Models
    from sklearn.neighbors import KNeighborsClassifier # KNN
    from sklearn.tree import DecisionTreeClassifier # ì˜ì‚¬ê²°ì •ë‚˜ë¬´
    from sklearn.linear_model import LogisticRegression # ë¡œì§€ìŠ¤í‹± íšŒê·€
    from sklearn.svm import SVC # ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜
    from sklearn.ensemble import RandomForestClassifier # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜
    from sklearn.ensemble import GradientBoostingClassifier # ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¶„ë¥˜
    from sklearn.naive_bayes import GaussianNB # ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ
    from xgboost import XGBRegressor # XGB íšŒê·€

    # Extras
    from sklearn.svm import NuSVC # Nu ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜
    from sklearn.svm import LinearSVC # ì„ í˜• ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜
    from sklearn.ensemble import AdaBoostClassifier # AdaBoost ë¶„ë¥˜
    from sklearn.ensemble import ExtraTreesClassifier # Extra Trees ë¶„ë¥˜
    from sklearn.ensemble import HistGradientBoostingClassifier # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¶„ë¥˜
    from sklearn.ensemble import BaggingClassifier # ë°°ê¹… ë¶„ë¥˜
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # ì„ í˜• íŒë³„ ë¶„ì„
    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # ì´ì°¨ íŒë³„ ë¶„ì„
    from sklearn.linear_model import RidgeClassifier # ë¦¿ì§€ ë¶„ë¥˜
    from sklearn.linear_model import Perceptron # í¼ì…‰íŠ¸ë¡ 
    from sklearn.neural_network import MLPClassifier # ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ë¶„ë¥˜
    from sklearn.gaussian_process import GaussianProcessClassifier # ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ë¶„ë¥˜
    from sklearn.naive_bayes import ComplementNB # ë³´ì™„ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ
    from sklearn.naive_bayes import BernoulliNB # ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ
    import xgboost as xgb # xgb (ë³„ì¹­)


    ```


êµì°¨ ê²€ì¦

    ```py
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.model_selection import cross_val_score

    # ì „ì²˜ë¦¬ê¸° naê°’ ìë™ì±„ì›€ê³¼
    # ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ëª¨ë¸ì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬ì¶•,
    # ë™ì¼í•œ ê²°ê³¼ë¥¼ ìœ„í•œ random_state=0
    my_pipe = Pipeline(steps=[
        ('preprocessor', SimpleImputer()) ,
        ('model', RandomForestRegressor(n_estimators=50, random_state=0))
    ])

    #neg_mab_error ì˜ ê²°ê³¼ëŠ” -ìœ¼ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— -1 ì„ ê³±í•´ì¤€ë‹¤.
    scores = -1 * cross_val_score(
        my_pipe, X, y,
        cv=4,
        scoring='neg_mean_absolute_error')

    print(scores.mean())

    ```


PCA

    ```py
    import pandas as pd
    from sklearn.decomposition import PCA
    import matplotlib.pyplot as plt

    train = pd.read_csv("table.csv")
    feature_df = train[train.columns[1:6]]
    pca = PCA(n_components = 2).fit_transform(feature_df)

    colors = {0:"blue", 1:"red"}
    c = train["attrition"].replace(colors)
    ```
    ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    ```py
    fig, ax = plt.subplots(figsize=(4,4))
    ax.scatter(pca[:,0], pca[:,1], alpha=0.6, color=c)
    ax.set(xlabel=R"X", ylabel=R"Y", title="PCA");
    ```


ê·¸ë¦¬ë“œ ì„œì¹˜, ëœë”ë§ˆì´ì¦ˆë“œ ì„œì¹˜

    ```py
    from sklearn.model_selection import GridSearchCV
    from sklearn.model_selection import RandomizedSearchCV
    from sklearn.model_selection import StratifiedKFold

    def grid_search(x_train, y_train, params, base_model):
        model_base = GridSearchCV( # or Randomized Search
            # n_iter=10 # for Randomized Search
            base_model,
            params,
            cv = StratifiedKFold(3,shuffle=True, random_state = 209), # Cross Valid
            return_train_score=True,
            n_jobs = -1 # CPU or GPU?
            )

        model_base.fit(x_train, y_train)

        best_model = model_base.best_estimator_
        best_pred = best_model.predict(x_test)
        print("ìµœê³  ì •í™•ë„", metrics.accuracy_score(best_pred,y_test))
        return best_model, grid_model.cv_results_ # ìµœê³ ì„±ëŠ¥ ëª¨ë¸ê³¼ ,êµì°¨ê²€ì¦ ê²°ê³¼

    params = {} # dict í˜•ì‹ {"íŒŒë¼ë¯¸í„°": list,}

    ```

<br>

<!------------------------------------------------------------------------------------------------------->

## ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ê¸°ë²•

### ë‚´ìš© ì •ë¦¬
ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ê¸°ë²• ì„¤ëª…
  
  ### ìƒ˜í”Œë§ ê¸°ë²•
  - ì„ì˜ ì¶”ì¶œ
  - ê³„í†µ ì¶”ì¶œ (ê³µì¥)
  - ì¸µí™” ì¶”ì¶œ (ë‚˜ì´ ë° ì„±ë³„ë³„ ì¶”ì¶œ)
  - êµ°ì§‘ ì¶”ì¶œ (ì „êµ­ -> ì„œìš¸)
  - ë‹¤ ë‹¨ê³„ ì¶”ì¶œ (ì „êµ­ -> ì„œìš¸ -> ë‚¨ì„±)
  - ë¹„ í™•ë¥ ì  ì¶”ì¶œ (ì„ì˜ ì¶”ì¶œ)
  
  ì£¼ì˜ : í¸í–¥ì ì¸ ë°ì´í„°ê°€ ë˜ì§€ ì•Šê²Œ
  
  

### codes
ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ê¸°ë²•
  
  ### ìƒ˜í”Œë§ ê¸°ë²• ì½”ë“œ
  
  ```py
  # ì–¸ë” ìƒ˜í”Œë§
  from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours
  from imblearn.over_sampling import RandomOverSampler, SMOTE
  from imblearn.combine import SMOTEENN
  
  RandomUnderSampler
  EditedNearestNeighbours 
  
  # ì˜¤ë²„ ìƒ˜í”Œë§
  RandomOverSampler
  SMOTE
  
  # Both
  SMOTEENN
  ```
  


<!------------------------------------------------------------------------------------------------------->

## ë”¥ëŸ¬ë‹
ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¡°

|ì´ë¦„|íŠ¹ì§•|êµ¬ì¡°|
|-|-|-|
|ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ |XOR ë¬¸ì œì™€ ê°™ì€ ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ìŒ<br>ì—­ì „íŒŒëŠ” ì¡´ì¬í•˜ì§€ ì•Šì•˜ë‹¤|ë‹¨ì¸µ êµ¬ì¡°|
|ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)|ë²”ìš© ê·¼ì‚¬ê¸°:<br>ì¶©ë¶„íˆ í¬ê³  ë³µì¡í•œ ì–´ë– í•œ ë¬¸ì œë¼ë„ ì´ë¡ ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥|ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ(ë‹¤ìˆ˜), ì¶œë ¥ì¸µ|
|CNN (Convolutional Neural Networks)|ê³µê°„ì  ê³„ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ë°ì´í„°ì˜ íŠ¹ì§• ì¶”ì¶œì— íƒì›”í•¨|Convolutional layer, Pooling layer, Fully Connected layer|
|RNN (Recurrent Neural Networks)|ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ì— ê°•ì ,<br>ì‹œê³„ì—´ ë° ìì—°ì–´ ì²˜ë¦¬ì— ìœ ìš©|Recurrent êµ¬ì¡°, Hidden state vector|
|LSTM (Long Short-Term Memory)|ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ë¨,<br>Forget-Input-Output Gate ë° Cell state(ê¸°ì–µ ì…€)ë¥¼ ì‚¬ìš©|LSTM Cell êµ¬ì¡°, Gates (Forget, Input, Output), Cell state|
|GRU (Gated Recurrent Unit)|LSTMì˜ ê²½ëŸ‰í™”ëœ ë³€í˜•,<br>ë” ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ê¸°ì–µ ì…€ ì—†ì´ Gateë§Œ ì‚¬ìš©|GRU Cell êµ¬ì¡°, Update Gate, Reset Gate|
|AutoEncoder|ë°ì´í„°ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ê³  ì¬ìƒì„±í•˜ì—¬ ë°ì´í„° ì••ì¶• ë° ë…¸ì´ì¦ˆ ì œê±°,<br>íŠ¹ì„± í•™ìŠµì— ì‚¬ìš©ë¨|Encoder -> Latent Space(z) -> Decoder|
|Transformer|Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë™ì‹œì ìœ¼ë¡œ ì²˜ë¦¬,<br>ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°|Self-Attention Mechanism, Encoder-Decoder êµ¬ì¡°, Multi-Head Attention, Position-wise Feed-Forward Networks|
|ResNet (Residual Networks)|Residual Blockì„ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê¹Šì€ ì‹ ê²½ë§ì„ í•™ìŠµ,<br>Gradient Vanishing ë¬¸ì œ ì™„í™”|Residual Block, Skip Connections, Convolutional Layers|
|EfficientNet|ëª¨ë¸ì˜ í¬ê¸°ì™€ ê³„ì‚° íš¨ìœ¨ì„±ì„ ì¡°ì •í•˜ê¸° ìœ„í•œ Compound Scaling ì‚¬ìš©,<br>ë†’ì€ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„± ì œê³µ|EfficientNet Blocks, Compound Scaling, Swish Activation Function|
|VAE (Variational Autoencoder)|ì ì¬ ê³µê°„ì˜ í™•ë¥  ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±,<br>ë°ì´í„°ì˜ í™•ë¥ ì  íŠ¹ì„±ì„ ëª¨ë¸ë§|Encoder, Latent Space (Probability Distribution), Decoder, Variational Objective|
|GAN (Generative Adversarial Network)|ìƒì„±ìì™€ íŒë³„ì ê°„ì˜ ê²½ìŸì„ í†µí•´ ë°ì´í„° ìƒì„±,<br>ì´ë¯¸ì§€ ìƒì„±, ë°ì´í„° ì¦ê°• ë“±ì— ì‚¬ìš©|Generator, Discriminator, Adversarial Training|



ë¹„ìš© í•¨ìˆ˜

### ë¹„ìš©í•¨ìˆ˜ ë° ì†ì‹¤í•¨ìˆ˜
- ì†ì‹¤ í•¨ìˆ˜ : ë°ì´í„° í¬ì¸íŠ¸ í•˜ë‚˜ì— ëŒ€í•œ ì˜¤ì°¨ í•¨ìˆ˜
- ë¹„ìš© í•¨ìˆ˜ : ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜¤ì°¨ í•¨ìˆ˜

|êµ¬ë¶„|ì´ë¦„|íŠ¹ì§•|êµ¬ì¡°|
|-|-|-|-|
|íšŒê·€ ë¬¸ì œ|ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ |XOR ê°™ì€ ë¹„ì„ í˜• ë¬¸ì œì— ëŒ€í•œ í•œê³„<br>ì—­ì „íŒŒëŠ” ì¡´ì¬í•˜ì§€ ì•Šì•˜ë‹¤|ë‹¨ì¸µ êµ¬ì¡°|
|-|MSE|ì œê³±, ì´ìƒì¹˜ì— ë¯¼ê°|$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$|
|-|MAE|ì ˆëŒ€ ê°’, ì´ìƒì¹˜ì— ë‘”ê°|$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} \lvert y - \hat{y} \lvert$|
|-|í—ˆë¸Œ ì†ì‹¤|MSE + MAE|MSE + MAE ì˜ êµ¬ì¡°|
|-|ë¡œê·¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„|ì´ìƒì¹˜ì— ë§¤ìš° ê°•í•¨|$\log - \cosh = \frac{1}{N} \sum^{N}_{i = 1} \log({\cosh (\hat{y}-y)})$|
|ë¶„ë¥˜ ë¬¸ì œ|Cross Entropy Error|ì´ì§„ ë¶„ë¥˜ : binary CEE<br>ë‹¤ì¤‘ ë¶„ë¥˜ : Categorical CEE|$CEE = -\sum_{k=1}^i t_k\text{log}\hat{y}$|
|-|íŒì§€ ì†ì‹¤|SVM ì—ì„œ ì‚¬ìš©<br>ë§ˆì§„ ì˜¤ë¥˜ ìµœì†Œí™”||
|-|ì œê³± íŒì§€ ì†ì‹¤|ì´ìƒì¹˜ì˜ ë¯¼ê°||
|-|í¬ì¹¼ ì†ì‹¤|ì˜¤ë‹µì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬||



í™œì„±í™” í•¨ìˆ˜

### ë¹„ìš©í•¨ìˆ˜ ë° ì†ì‹¤í•¨ìˆ˜
- ì†ì‹¤ í•¨ìˆ˜ : ë°ì´í„° í¬ì¸íŠ¸ í•˜ë‚˜ì— ëŒ€í•œ ì˜¤ì°¨ í•¨ìˆ˜
- ë¹„ìš© í•¨ìˆ˜ : ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜¤ì°¨ í•¨ìˆ˜
- ì¢…ë¥˜ :
    |ì´ë¦„|ê³µì‹|ì¶œë ¥ ë²”ìœ„
    |-|-|-|
    |Sigmoid|$\phi = \frac{1}{1+e^{-x}}$|0 ~ 1|
    |tanh|$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$|-1 ~ 1|
    |ReLU|$f(z) = max(0, z)$|$0 \leq f(x)$|
    |Leaky ReLU|$f(z) = max(\epsilon z, z)$|$0 \leq f(x)$|
    |ELU|$f(x) = x \space \text{if } x \geq 0$<br>$f(x) = \alpha (e^x - 1) \space \text{if } x < 0$|$0 \leq f(x)$|
    |SoftPlus|$f(z) =  \ln(1 + e^x)$|$0 \leq f(x)$|
    |GeLU|$0.5 \cdot x \cdot \left( 1 + \tanh \left( \sqrt{\frac{2}{\pi}} \cdot \left( x + 0.044715 \cdot x^3 \right) \right) \right)$|Free <br>ReLU ê³„ì—´ ê·¸ë˜í”„ì™€ ë¹„ìŠ·|



ì˜µí‹°ë§ˆì´ì €

### ì˜µí‹° ë§ˆì´ì €
- ì˜µí‹° ë§ˆì´ì € : ìˆ˜ì¹˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- ì¢…ë¥˜ :
    |ì´ë¦„|í•™ìŠµë¥ |íƒìƒ‰ ë°©í–¥|ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜|
    |-|-|-|-|
    |SGD|ìƒìˆ˜|ê¸°ìš¸ê¸°|íƒìƒ‰ ë°©í–¥
    |Momentum|ìƒìˆ˜|ë‹¨ê¸° ëˆ„ì  ê¸°ìš¸ê¸°|íƒìƒ‰ ë°©í–¥
    |AdaGrad|ì¥ê¸° íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ê³¼ ë°˜ë¹„ë¡€|ê¸°ìš¸ê¸°|í•™ìŠµ ë¥ 
    |RMSProp|ë‹¨ê¸° íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ê³¼ ë°˜ë¹„ë¡€|ê¸°ìš¸ê¸°|í•™ìŠµ ë¥ 
    |Adam|ë‹¨ê¸° íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ê³¼ ë°˜ë¹„ë¡€|ë‹¨ê¸° ëˆ„ì  Grad|í•™ìŠµ ë¥ 



ë”¥ëŸ¬ë‹ ë¬¸ì œ í•´ê²° ê¸°ë²•

### ë¬¸ì œ ë° ì™„í™”ë²•
- ê²½ì‚¬ ì†Œì‹¤ ë¬¸ì œ
    - ReLU ê³„ì—´ì˜ í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš© <br> (Dead ReLU ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ)
- ê³¼ì í•© ë¬¸ì œ
     |ì´ë¦„|ë‚´ìš©|
    |-|-|
    |L1 ê·œì œ|ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ê°’ê³¼ ë¹„ë¡€í•˜ëŠ” ë¹„ìš© ì¶”ê°€<br>ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ íŠ¹ì„±ì— ëŒ€í•œ ì˜í–¥ ì œê±°<br>(ëª¨ë¸ì˜ í¬ì†Œì„± ì¦ê°€)|
    |L2 ê·œì œ|ê°€ì¤‘ì¹˜ì˜ ì œê³±ì— ë¹„ë¡€í•˜ëŠ” ë¹„ìš© ì¶”ê°€<br>ê°€ì¤‘ì¹˜ì˜ ê°’ì„ ì¤„ì—¬ ë³µì¡ì„±ì„ ë‚®ì¶˜ë‹¤<br>(ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€)<br>|
    |ë“œë¡­ ì•„ì›ƒ|í•™ìŠµ ê³¼ì • ì¤‘ ë…¸ë“œë¥¼ ì„ì˜ë¡œ ë¹„í™œì„±|
    |Early Stop|ë” ì´ìƒ í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•Šì„ë–„ í•™ìŠµ ì¤‘ë‹¨|
    |ë°ì´í„° ì¦ê°•|ë¹„ìŠ·í•œ ë°ì´í„°ë¥¼ ë³µì œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¡œ ë§Œë“¬<br>í…ŒìŠ¤íŠ¸ í• ë–„ ì¦ê°• ê¸ˆì§€|



### codes
                                                                        
ë‹¤ì–‘í•œ layers

    - ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
        ```py
        import tensorflow as tf
        from tensorflow.keras import datasets, layers, models, optimizers
        ```
    - CNN
        ```py
        model = models.Sequential()
        model.add(layers.Conv2D(32, (3, 3), activation='relu',
                            input_shape=input_shape))
        model.add(layers.MaxPooling2D(pool_size=(2, 2)))
        model.add(layers.Dropout(0.25))

        model.add(layers.Flatten())
        model.add(layers.Dense(512, activation='relu'))
        model.add(layers.Dropout(0.5))
        model.add(layers.Dense(classes, activation='softmax'))
        ```
    -  RNN
        ```py
        model = models.Sequential()
        model.add(layers.Embedding(max_features, 64, input_length=max_len))
        model.add(layers.SimpleRNN(32, activation='tanh', return_sequences=False))
        model.add(layers.Dense(16, activation='tanh'))
        model.add(layers.Dense(2, activation = 'softmax'))
        model.summary()
        ```
    - LSTM
        ```py
        model1 = models.Sequential()
        model1.add(layers.Embedding(max_features, 128, input_length=maxlen))

        # return_sequence=True - ëª¨ë“  ìŠ¤í…Œì´íŠ¸ë¥¼ ë‚´ë³´ëƒ„
        model1.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))
        model1.add(layers.Bidirectional(layers.LSTM(64)))
        model1.add(layers.Dense(2, activation = 'softmax'))
        model1.summary()
        ```
    


Auto Encoder

    ```py
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Dense, Input, Embedding, Flatten
    from tensorflow.keras.datasets import mnist
    from tensorflow.keras.regularizers import l1 # ì •ê·œí™” ê³¼ì í•© ë°©ì§€
    from tensorflow.keras.optimizers import Adam

    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    ```

    ```py
    input_size = 784
    hidden_size = 128
    code_size = 32 # ì ì¬ ê³µê°„ ë²¡í„°ì˜ í¬ê¸°

    input_img = Input(shape=(input_size,)) # ì¸í’‹

    hidden_1 = Dense(hidden_size, activation='relu')(input_img) # ì¸ì½”ë” ë¶€ë¶„

    code = Dense(code_size, activation='relu')(hidden_1) # ì ì¬ ê³µê°„

    hidden_2 = Dense(hidden_size, activation='relu')(code) # ë””ì½”ë” ë¶€ë¶„(ì¸ì½”ë”ì™€ ê°™ë‹¤)
    output_img = Dense(input_size, activation='sigmoid')(hidden_2)
    # ì¸ì½”ë” ë¶€ë¶„ê³¼ ë””ì½”ë” ë¶€ë¶„ ë‘˜ë‹¤ ìˆì–´ì•¼ í•œë‹¤ë©´,
    # ì¶œë ¥ ì¸µì˜ ì‚¬ì´ì¦ˆëŠ” ì…ë ¥ì¸µì˜ ì‚¬ì´ì¦ˆì™€ ê°™ì•„ì•¼ í•œë‹¤.

    # ì¸ì½”ë”ë¶€ë¶„ê³¼ ë””ì½”ë” ë¶€ë¶„ì˜ ê²°í•©
    autoencoder = Model(input_img, output_img)
    ```
    



seq2seq

    ```py
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, LSTM, Dense

    # ì¸ì½”ë”
    encoder_inputs = Input(shape=(None, 50))
    encoder_lstm = LSTM(256, return_state=True)
    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)
    encoder_states = [state_h, state_c]

    # ë””ì½”ë”
    decoder_inputs = Input(shape=(None, 50))
    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)

    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
    decoder_dense = Dense(50, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    # ëª¨ë¸ ì»´íŒŒì¼
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # ëª¨ë¸ ìš”ì•½
    model.summary()
    ```
    ```

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
    â”ƒ Layer (type)        â”ƒ Output Shape      â”ƒ    Param # â”ƒ Connected to      â”ƒ
    â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
    â”‚ input_layer         â”‚ (None, None, 50)  â”‚          0 â”‚ -                 â”‚
    â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ input_layer_1       â”‚ (None, None, 50)  â”‚          0 â”‚ -                 â”‚
    â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ lstm (LSTM)         â”‚ [(None, 256),     â”‚    314,368 â”‚ input_layer[0][0] â”‚
    â”‚                     â”‚ (None, 256),      â”‚            â”‚                   â”‚
    â”‚                     â”‚ (None, 256)]      â”‚            â”‚                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ lstm_1 (LSTM)       â”‚ [(None, None,     â”‚    314,368 â”‚ input_layer_1[0]â€¦ â”‚
    â”‚                     â”‚ 256), (None,      â”‚            â”‚ lstm[0][1],       â”‚
    â”‚                     â”‚ 256), (None,      â”‚            â”‚ lstm[0][2]        â”‚
    â”‚                     â”‚ 256)]             â”‚            â”‚                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ dense (Dense)       â”‚ (None, None, 50)  â”‚     12,850 â”‚ lstm_1[0][0]      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    


Transformer

    ```py
    from tensorflow.keras import layers

    class EncoderBlock(layers.Layer):
        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
            super().__init__()
            self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
            self.ffn = keras.Sequential(
                [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
            )

            # ë ˆì´ì–´ ì •ê·œí™”
            self.layernorm1 = layers.LayerNormalization()
            self.layernorm2 = layers.LayerNormalization()

            self.dropout1 = layers.Dropout(rate)
            self.dropout2 = layers.Dropout(rate)

        def call(self, inputs, training):
            attn_output = self.att(inputs, inputs)
            attn_output = self.dropout1(attn_output, training=training)
            out1 = self.layernorm1(inputs + attn_output)
            ffn_output = self.ffn(out1)
            ffn_output = self.dropout2(ffn_output, training=training)
            return self.layernorm2(out1 + ffn_output)
    ```

    ```py
    # í† í° ë° ìœ„ì¹˜ ì„ë² ë”© ì •ì˜
    class TokenAndPositionEmbedding(layers.Layer):
        def __init__(self, maxlen, vocab_size, embed_dim):

            super().__init__()
            self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
            self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

        def call(self, x):
            maxlen = tf.shape(x)[-1]
            positions = tf.range(start=0, limit=maxlen, delta=1)
            positions = self.pos_emb(positions)
            x = self.token_emb(x)
            return x + positions
    ```

    ```py
    # ëª¨ë¸ ì„¤ê³„

    embed_dim = 32  # ê° í† í°ì˜ ì„ë² ë”© ë²¡í„° í¬ê¸°
    num_heads = 2  # ì–´í…ì…˜ í—¤ë“œì˜ ìˆ˜
    ff_dim = 32  # ì™„ì „ì—°ê²°ì¸µì˜ ë…¸ë“œ ìˆ˜

    inputs = layers.Input(shape=(maxlen,))
    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
    x = embedding_layer(inputs)
    x = EncoderBlock(embed_dim, num_heads, ff_dim)(x, training=True)
    x = EncoderBlock(embed_dim, num_heads, ff_dim)(x, training=True)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(0.1)(x)
    x = layers.Dense(20, activation="relu")(x)
    x = layers.Dropout(0.1)(x)
    outputs = layers.Dense(2, activation="softmax")(x)
    ```

    ```py
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.summary()
    ```

    ```py
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
    â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ
    â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
    â”‚ input_layer_10 (InputLayer)     â”‚ (None, 200)            â”‚             0 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ token_and_position_embedding_2  â”‚ (None, 200, 512)       â”‚    10,342,400 â”‚
    â”‚ (TokenAndPositionEmbedding)     â”‚                        â”‚               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ encoder_block_8 (EncoderBlock)  â”‚ (None, 200, 512)       â”‚     6,336,544 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ global_average_pooling1d_2      â”‚ (None, 512)            â”‚             0 â”‚
    â”‚ (GlobalAveragePooling1D)        â”‚                        â”‚               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ dropout_31 (Dropout)            â”‚ (None, 512)            â”‚             0 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ dense_22 (Dense)                â”‚ (None, 20)             â”‚        10,260 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ dropout_32 (Dropout)            â”‚ (None, 20)             â”‚             0 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ dense_23 (Dense)                â”‚ (None, 2)              â”‚            42 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```

<!------------------------------------------------------------------------------------------------------->

## ë°ì´í„° ì¦ê°• ê¸°ë²•

keras ë³€í˜• ì¦ê°•

    ```py
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img

    # ë³„í‘œ ìœ„ì£¼ë¡œ ì“°ì„
    tf.keras.preprocessing.image.ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        zca_whitening=False,
        zca_epsilon=1e-06,
        rotation_range=0, #***
        width_shift_range=0.0, #***
        height_shift_range=0.0, #***
        brightness_range=None, #***
        shear_range=0.0,
        zoom_range=0.0, #***
        channel_shift_range=0.0,
        fill_mode='nearest',
        cval=0.0,
        horizontal_flip=False, #***
        vertical_flip=False, #***
        rescale=None,
        preprocessing_function=None,
        data_format=None,
        validation_split=0.0,
        interpolation_order=1,
        dtype=None
    )
    ```

    ```py
    # ì´ë¯¸ì§€ ë¡œë“œ (ì˜ˆì‹œë¡œ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©)
    img = load_img('image.jpg')  # ì´ë¯¸ì§€ ê²½ë¡œ
    x = img_to_array(img)  # ì´ë¯¸ì§€ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
    x = x.reshape((1,) + x.shape)  # (1, height, width, channels) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

    ```


AE í•™ìŠµ ì¦ê°•

    ```py
    import os
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
    from tensorflow.keras.models import Model
    from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img
    ```
    - í•¨ìˆ˜ì •ì˜
        ```py
        # ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬ - ë¡œì»¬
        def load_images_local(folder_path, target_size=(128, 128)):
            images = []
            filenames = os.listdir(folder_path)
            for filename in filenames:
                try:
                    img_path = os.path.join(folder_path, filename)
                    img = load_img(img_path, target_size=target_size)
                    img = img_to_array(img) / 255.0
                    images.append(img)
                except:
                    pass
            return np.array(images)

            # ì´ë¯¸ì§€ ì¦ê°• ë° ì €ì¥
        def augment_images(autoencoder, images, save_dir):
            decoded_images = autoencoder.predict(images)
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)
            for i, img_array in enumerate(decoded_images):
                img = array_to_img(img_array)
                img.save(os.path.join(save_dir, f'augmented_image_{i}.png'))
        ```
    - ì¦ê°• ì‹œì‘
        ```py
        data_folder = '../data/data_mw/woman'  # ì´ë¯¸ì§€ í´ë”
        save_folder = '../data/data_mw_add/woman_new'  # ì¦ê°•ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë”

        # ì´ë¯¸ì§€ ë¡œë“œ - ë¡œì»¬
        images = load_images_local(data_folder)

        # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ êµ¬ì„± ë° í›ˆë ¨
        autoencoder = build_autoencoder(input_shape=(128, 128, 3))
        autoencoder.fit(images, images, epochs=20, batch_size=20)

        # ì´ë¯¸ì§€ ì¦ê°• ë° ì €ì¥
        augment_images(autoencoder, images, save_folder)
        ```




## ë‹¤ì–‘í•œ Pretraind ëª¨ë¸
CNN ê¸°ë°˜

|ì´ë¦„|ë‚´ìš©|íŠ¹ì§•|ë ˆì´ì–´|
|-|-|-|-|
|LeNet|CNN ì´ˆê¸° ëª¨ë¸|ì–€ ë¥´ì¿¤ì— ì˜í•´ ê°œë°œ, ì†ê¸€ì”¨ ì¸ì‹ì— ì‚¬ìš©|ê¸°ë³¸ CNN êµ¬ì¡° (Convolutional Layers, Pooling Layers)|
|AlexNet|ReLU í™œì„±í™” í•¨ìˆ˜, ë°ì´í„° ì¦ê°•, MaxPoolingì„ í†µí•œ ë²¡í„°í™”, ë“œë¡­ì•„ì›ƒ, ë‹¤ì¤‘ GPU í™œìš©|ReLU í™œìš©, ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ|Convolutional Layers, ReLU, MaxPooling, Dropout|
|VGG-16|3x3 í•„í„°ì™€ 2x2 MaxPooling í™œìš©, êµ¬ì¡° ë‹¨ìˆœí™”, ê·œì œ ê¸°ë²• ì ìš©|ì˜¥ìŠ¤í¬ë“œ VGG ê·¸ë£¹ì— ì˜í•´ ê°œë°œ, ê¹Šì´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬|Convolutional Layers (3x3), MaxPooling (2x2), Fully Connected Layers|
|InceptionNet<br>(Google Net)|Bottle neck êµ¬ì¡°, Inception Module, Auxiliary classifier, Main classifier|Googleì— ì˜í•´ ê°œë°œ, 1x1 í•„í„°ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ|Inception Modules, 1x1, 3x3, 5x5 Convolutions, Pooling|
|ResNet|Residual blockì„ í†µí•œ Skip Connection, ê²½ì‚¬ ì†Œì‹¤ ë¬¸ì œ ì™„í™”|Microsoftì— ì˜í•´ ê°œë°œ, VGG-19ì˜ ë¼ˆëŒ€, Residual Blocks ì‚¬ìš©|Residual Blocks, Skip Connections, Convolutional Layers|
|MobileNet|Depthwise Separable Convolution, ê° ì±„ë„ë³„ë¡œ ë…ë¦½ì ì¸ ì—°ì‚° í›„ í†µí•©|Googleì˜ Howardì— ì˜í•´ ê°œë°œ, ì„±ëŠ¥ ìœ ì§€ ë° ì†ë„ í–¥ìƒ|Depthwise Separable Convolutions, 1x1 Convolutions|
|DenseNet|Dense Block êµ¬ì¡°, ëª¨ë“  ë ˆì´ì–´ì˜ inputì„ outputì— Concat|ResNetê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥, Feature ì¬ì‚¬ìš© ì¦ê°€|Dense Blocks, Convolutional Layers, Concatenation|
|EfficientNet|ìµœì ì˜ Depth, Width, Resolutionì„ ì°¾ê¸° ìœ„í•œ Grid Search, íš¨ìœ¨ì ì¸ ëª¨ë¸ í¬ê¸° ë° ì„±ëŠ¥|êµ¬ê¸€ì— ì˜í•´ ê°œë°œ, ëª¨ë¸ í¬ê¸°ì™€ ê³„ì‚° íš¨ìœ¨ì„± ìµœì í™”|Compound Scaling, Convolutional Layers, EfficientNet Blocks|




ìì—°ì–´ ì²˜ë¦¬ ê¸°ë°˜

|ì´ë¦„|ë‚´ìš©|íŠ¹ì§•|
|-|-|-|
|Transformer|Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë™ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©°, ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëª¨ë¸|Self-Attention, Multi-Head Attention, Encoder-Decoder êµ¬ì¡°|
|BERT (Bidirectional Encoder Representations from Transformers)|ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì´í•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ ëª¨ë¸. Masked Language Modelingê³¼ Next Sentence Predictionì„ í†µí•´ ì‚¬ì „ í•™ìŠµë¨|Bidirectional Context, Pre-training and Fine-tuning, ë‹¤ì–‘í•œ NLP ì‘ì—…ì— í™œìš©|
|GPT (Generative Pre-trained Transformer)|ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, ì–¸ì–´ ìƒì„±ê³¼ ë²ˆì—­ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ NLP ì‘ì—…ì— ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜. Transformer ê¸°ë°˜ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ì‚¬ì „ í•™ìŠµë¨|Unidirectional Context, Language Modeling, Transfer Learning|



ê°ì²´ íƒì§€ ëª¨ë¸

|Shots|ì´ë¦„|ë‚´ìš©|íŠ¹ì§•|
|-|-|-|-|
|Two|R-CNN<br>(Regions with CNN features)|ì „í†µì ì¸ ê°ì²´ íƒì§€ ë°©ë²•:<br>Selective Searchë¡œ ì˜ì—­ì„ ì œì•ˆ-><br>CNNìœ¼ë¡œ í”¼ì²˜ ë²¡í„°ë¡œ ë³€í™˜-><br>ë¶„ë¥˜ ë° ê²½ê³„ ìƒìë¥¼ ì˜ˆì¸¡|Two-stage detector,<br>Selective Search,<br>CNN-based feature extraction|
|Two|Fast R-CNN|R-CNNì˜ ê°œì„ , ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•´ CNNì„ í•œ ë²ˆë§Œ ì‹¤í–‰,<br>RoI Poolingë¡œ ê° ì œì•ˆ ì˜ì—­ì˜ í”¼ì²˜ë¥¼ ì¶”ì¶œ ë¶„ë¥˜ ë° íšŒê·€|RoI Pooling,<br>End-to-end training,<br>Faster processing compared to R-CNN|
|Two|Faster R-CNN|Region Proposal Network (RPN)ê³¼<br>Fast R-CNNì„ ê²°í•©|RPN for region proposals,<br>ROI Pooling|
|One|YOLO<br>(You Only Look Once)|One-Shot. ë¹ ë¥¸ ì†ë„ì™€ ë†’ì€ ì‹¤ì‹œê°„ ì„±ëŠ¥|Bounding box regression,<br>Class prediction|
|One|SSD<br>(Single Shot MultiBox Detector)|ë‹¤ì–‘í•œ í¬ê¸° ê°ì²´ë¥¼ íƒì§€<br>ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì„±ì„ í™œìš©|Multi-scale feature maps,<br>Default boxes|

> RoI : Region of interest



### codes
- ì´ë¯¸ì§€ ë¶„ë¥˜
    ì´ë¯¸ì§€ ê¸°ë³¸ ì „ì²˜ë¦¬

        ```py
        import tensorflow as tf
        from tensorflow.keras.preprocessing.image import ImageDataGenerator

        # ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬
        def preprocess_image(image_size=(224, 224), batch_size=32):
            datagen = ImageDataGenerator(
                rescale=1./255,
                shear_range=0.2,
                zoom_range=0.2,
                horizontal_flip=True,
                validation_split=0.2
            )

            train_generator = datagen.flow_from_directory(
                'path/to/dataset',
                target_size=image_size,
                batch_size=batch_size,
                class_mode='categorical',
                subset='training'
            )

            validation_generator = datagen.flow_from_directory(
                'path/to/dataset',
                target_size=image_size,
                batch_size=batch_size,
                class_mode='categorical',
                subset='validation'
            )

            return train_generator, validation_generator

        ```
    

    ë‹¤ì–‘í•œ CNN based models

        ```py
        from tensorflow.keras.applications import (
            VGG16, VGG19,
            ResNet50, ResNet101, ResNet152, 
            InceptionV3, InceptionResNetV2,
            Xception,
            DenseNet121, DenseNet169, DenseNet201,
            MobileNet, MobileNetV2, MobileNetV3Small, MobileNetV3Large,
            NASNetMobile, NASNetLarge,
            EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, 
            EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7
        )
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
        
        # ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        models_dict = {
            'VGG16': VGG16,
            'VGG19': VGG19,
            'ResNet50': ResNet50,
            'ResNet101': ResNet101,
            'ResNet152': ResNet152,
            'InceptionV3': InceptionV3,
            'InceptionResNetV2': InceptionResNetV2,
            'Xception': Xception,
            'DenseNet121': DenseNet121,
            'DenseNet169': DenseNet169,
            'DenseNet201': DenseNet201,
            'MobileNet': MobileNet,
            'MobileNetV2': MobileNetV2,
            'MobileNetV3Small': MobileNetV3Small,
            'MobileNetV3Large': MobileNetV3Large,
            'NASNetMobile': NASNetMobile,
            'NASNetLarge': NASNetLarge,
            'EfficientNetB0': EfficientNetB0,
            'EfficientNetB1': EfficientNetB1,
            'EfficientNetB2': EfficientNetB2,
            'EfficientNetB3': EfficientNetB3,
            'EfficientNetB4': EfficientNetB4,
            'EfficientNetB5': EfficientNetB5,
            'EfficientNetB6': EfficientNetB6,
            'EfficientNetB7': EfficientNetB7
        }
        
        # ëª¨ë¸ ìƒì„± í•¨ìˆ˜
        def create_model(model_name, input_shape=(224, 224, 3), num_classes=1000):
            base_model = models_dict[model_name](weights='imagenet', include_top=False, input_shape=input_shape)
        
            # ëª¨ë¸ êµ¬ì¡° ì •ì˜
            model = Sequential()
            model.add(base_model)
            model.add(GlobalAveragePooling2D())
            model.add(Dense(1024, activation='relu'))
            model.add(Dense(num_classes, activation='softmax'))
            
            return model
        
        # ëª¨ë¸ ìƒì„± ì˜ˆì‹œ
        model_name = 'ResNet50'
        input_shape = (224, 224, 3)
        num_classes = 10  # ë°ì´í„°ì…‹ì— ë”°ë¥¸ í´ë˜ìŠ¤ ìˆ˜
        model = create_model(model_name, input_shape, num_classes)
        
        # ëª¨ë¸ ìš”ì•½ ì¶œë ¥
        model.summary()
        
        ```

        
- LM Models
    BERTopic
        : í…ìŠ¤íŠ¸ì˜ í† í”½ ì¶”ì¶œ ë° ì‹œê°í™” - íŠ¸ëœìŠ¤ í¬ë¨¸ ê¸°ë°˜, ëŒ€ëŸ‰ ë¬¸ì„œ ìë™ í† í”½ ì¶”ì¶œ, í† í”½ ì‚¬ì´ì˜ ê´€ê³„ íŒŒì•…<br>
        : ì£¼ìš” ê¸°ëŠ¥ - ìë™ í† í”½ ìˆ˜ ê²€ì¶œ, ìœ ì‚¬í•œ í† í”½ ì œê±°, ì‹œê°í™”, ë™ì  í† í”½ ëª¨ë¸ë§(ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” íŠ¸ë Œë“œ ì¶”ì²™)<br>

        ```py
        !pip install update BERTopic
        ``` 

        ```py
        from bertopic import BERTopic

        import pandas as pd
        import numpy as np

        # ë°ì´í„° ì½ì–´ì˜¤ê¸°
        df = pd.read_csv('topic_example.csv', engine='python')

        # ë‚´ìš© ì²˜ë¦¬ í•˜ê¸°
        docs = df['text'].to_list()
        docs = [re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£ .]', '', s) for s in docs]
        docs = [re.sub(r'\s+', ' ', s) for s in docs]

        model = BERTopic(language='korean', nr_topics=10, calculate_probabilities=True)
        topics, probabilities = model.fit_transform(docs)

        # í† í”½ ìš”ì•½
        model.get_topic_info()

        # í† í”½ (3) ì˜ ìƒì„¸ ë‚´ìš© í™•ì¸
        model.get_topic(3)

        # ë‹¤ì–‘í•œ ì‹œê°í™”
        model.visualize_barchart(top_n_topics=8) # ë°” ì°¨íŠ¸
        model.visualize_topics() # ì£¼ì œê°„ ê±°ë¦¬ ì°¨íŠ¸
        model.visualize_hierarchy(top_n_topics=10) # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§
        model.visualize_heatmap(top_n_topics=10) # íˆíŠ¸ë§µ
        model.visualize_distribution(model.probabilities_[0], min_probability=0.015) # íŠ¹ì • ë¬¸ì„œ ì£¼ì œ ë¶„í¬ ì‹œê°í™”
        ```
        ì™¸ë¶€ ëª¨ë¸
        ```py
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        topic_model = BERTopic(embedding_model=embedding_model)

        topics, probabilities = topic_model.fit_transform(docs)

        # ì„ë² ë”© ë²¡í„° ë§Œë“¤ê¸°
        embeddings = embedding_model.encode(docs)
        print("ì„ë² ë”© ì°¨ì›:", embeddings.shape)
        print("ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì„ë² ë”© ë²¡í„°:", embeddings[0])
        ```
        
    
        
    GPT

        ```py
        import torch
        from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast

        tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
        bos_token='</s>', eos_token='</s>', unk_token='<unk>',
        pad_token='<pad>', mask_token='<mask>')

        model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
        text = '''ìš°ë¦¬ ì‚¶ì— ê°€ì¥ í•„ìš”í•œ ë•ëª©ì€ ë¬´ì—‡ì¼ê¹Œ? ê·¸ê±´ ë°”ë¡œ ì·¨'''

        input_ids = tokenizer.encode(text, return_tensors='pt')
        gen_ids = model.generate(input_ids,
                                   max_length=100,
                                   repetition_penalty=2.0,
                                   pad_token_id=tokenizer.pad_token_id,
                                   eos_token_id=tokenizer.eos_token_id,
                                   bos_token_id=tokenizer.bos_token_id,
                                   use_cache=True)
        generated = tokenizer.decode(gen_ids[0])
        print(generated)
        ```
        ```
        ìš°ë¦¬ ì‚¶ì— ê°€ì¥ í•„ìš”í•œ ë•ëª©ì€ ë¬´ì—‡ì¼ê¹Œ? ê·¸ê±´ ë°”ë¡œ ì·¨ì—…ê³¼ ìŠ¹ì§„ì´ë‹¤.
        ê·¸ëŸ°ë° ì´ê²Œ ì™œ ì¤‘ìš”í•œì§€ ì•Œ ìˆ˜ ì—†ë‹¤.
        ì´ë ‡ê²Œ ì·¨ì—…ë‚œì— í—ˆë•ì´ëŠ” ì²­ë…„ë“¤ì´ ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì€ ì§ì¥ì„ êµ¬í• ê¹Œ ê³ ë¯¼í•˜ëŠ” ê²ƒì€ ë‹¹ì—°í•œ ì¼ì´ë‹¤.
        í•˜ì§€ë§Œ ì´ëŸ° ê³ ë¯¼ì„ í•˜ëŠ” ì´ìœ ëŠ” ë­˜ê¹Œ?
        ë°”ë¡œ 'ì·¨ì—…' ë•Œë¬¸ì´ë‹¤.
        ì·¨ì—…ì„ ìœ„í•´ì„  ë¬´ì—‡ë³´ë‹¤ ìì‹ ì˜ ì ì„±ê³¼ ëŠ¥ë ¥ì— ë§ëŠ” ì¼ìë¦¬ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.
        ê·¸ë˜ì•¼ ìì‹ ì´ ì›í•˜ëŠ” ì§ì¥ì— ê°ˆ í™•ë¥ ì´ ë†’ì•„ì§„ë‹¤.
        ë˜í•œ ìê¸°ê³„ë°œì„ ìœ„í•œ ë…¸ë ¥ë„ í•„ìš”í•˜ë‹¤.
        ìì‹ ì˜ ëŠ¥ë ¥ì„ ìµœëŒ€í•œ ë°œíœ˜í•  ê¸°íšŒë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´
        ```
    

- [Object Detection Models](__KDT__season_3/09_Object_Detection/)

## ëª¨ë¸ í‰ê°€ í•˜ê¸°

### ë‚´ìš© ì •ë¦¬

ëª¨ë¸ í‰ê°€
  
  1. ì •í™•ë„(Accuracy):
      - ì¼ë°˜ì  í‰ê°€ ì§€í‘œ
      - ë°ì´í„°ê°€ ê· í˜•
  
  2. ì¬í˜„ìœ¨(Recall), ì •ë°€ë„(Precision), F1-score:
      - ë°ì´í„°ì˜ ë¶ˆê· í˜• 30% ì´ìƒì¼ë•Œ
      - ì¬í˜„ìœ¨: ì‹¤ì œ ì–‘ì„± ì¤‘ ì–‘ì„± ë¹„ìœ¨ <br>
         : $$\frac{\text{TP}}{\text{FP} + {\text{FP}}}$$
      - ì •ë°€ë„: ì˜ˆì¸¡í•œ ì–‘ì„± ì¤‘ ì‹¤ì œ ì–‘ì„±ë¹„ìœ¨ <br>
         : $$\frac{\text{TP}}{\text{FP} + {\text{FN}}}$$
      - F1-score: ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ì¡°í™” í‰ê· 
  
  3. í˜¼ë™ í–‰ë ¬(Confusion Matrix):
      - ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ê´€ê³„ë¥¼ ë³´ì—¬ì¤Œ
      - ì •í™•ë„, ì¬í˜„ìœ¨, ì •ë°€ë„ ì§€í‘œ
      - ROC ê³¡ì„  ë° AUC(Area Under the Curve):
          - ì´ì§„ ë¶„ë¥˜ ì‹œ í‰ê°€ Metric
          - ì„ê³„ê°’ì— ë”°ë¥¸ True Positive Rateì™€ False Positive Rateë¥¼ ë‚˜íƒ€ëƒ„
          - AUC ê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ìŒ, 0.5 ëŠ” ë„˜ì–´ì•¼ í•¨
          - AUC = 0.5 -> ëœë¤ ë¶„ë¥˜ê¸°ì™€ ì„±ëŠ¥ì´ ê°™ë‹¤.
  
  4. R-squared(R2-Score):
  
      - íšŒê·€ ëª¨ë¸ í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ì§€í‘œ
      - ëª¨ë¸ì´ ì¢…ì†ë³€ìˆ˜ì˜ ë³€ë™ì„ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ„
      - 0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ìŒ

  5. êµì°¨ ê²€ì¦(Cross-Validation):
  
      - kê°œì˜ í´ë“œ(fold)ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ í‰ê°€
      - í›ˆë ¨ ,ê²€ì¦ ë°ì´í„°ë¥¼ ë¶„ë¦¬ í›„ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€
      - ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ , ì•ˆì •ì„±ì„ í™•ì¸
  
  
  
  6. ë„ë©”ì¸ ì§€ì‹ í™œìš©:
  
      - ë°ì´í„°ì— ëŒ€í•œ ë„ë©”ì¸ ì´í•´ ë° í‰ê°€

<!-------------------------------------------------------------------------------------------------------> 

## ë¶„ë¥˜ ë° íšŒê·€ ë¬¸ì œ
ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë¶„ë¥˜ íšŒê·€ ë¬¸ì œ ìœ í˜•

### ë¶„ë¥˜ ë¬¸ì œ
|ì´ë¦„|ë‚´ìš©|
|-|-|
|Mnist|ì† ê¸€ì”¨ ë¶„ë¥˜|
|CIFAR|ì‚¬ì§„ ëŒ€ìƒ ë¶„ë¥˜|
|í…ìŠ¤íŠ¸, í‘œì •, ê°ì„±|ì£¼ë¡œ ì‹œí€€ìŠ¤ context í•´ì„ ë¬¸ì œ|
|ì¼ ëŒ€ ë‹¤ ë¶„ë¥˜|ë‹¨ê³„ë³„ë¡œ í•˜ë‚˜ì”© ë¶„ë¥˜|

### íšŒê·€ ë¬¸ì œ
|ì´ë¦„|ë‚´ìš©|
|-|-|
|ì£¼íƒ ê°€ê²© ì˜ˆì¸¡|ê°€ê²© ì˜ˆì¸¡|
|ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡|ê°€ê²© ì˜ˆì¸¡|
|ì˜¨ë„ ì˜ˆì¸¡|ê¸°ìƒ ë°ì´í„°ë¡œ ì˜¨ë„ ì˜ˆì¸¡|



<!-------------------------------------------------------------------------------------------------------> 

## ì‹œê³„ì—´
ì‹œê³„ì—´ ì´ë¡ 


<!-------------------------------------------------------------------------------------------------------> 
